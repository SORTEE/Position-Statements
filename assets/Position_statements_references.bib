
@article{rosenthal1979,
	title = {The file drawer problem and tolerance for null results},
	volume = {86},
	issn = {1939-1455},
	doi = {10.1037/0033-2909.86.3.638},
	abstract = {For any given research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type I errors, while the file drawers are filled with the 95\% of the studies that show nonsignificant results. Quantitative procedures for computing the tolerance for filed and future null results are reported and illustrated, and the implications are discussed. (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Rosenthal, Robert},
	year = {1979},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Experimentation, Scientific Communication, Statistical Probability, Statistical Tests, Type I Errors},
	pages = {638--641},
	file = {Snapshot:/Users/joelpick/Zotero/storage/Q2XDFDDF/doiLanding.html:text/html},
}

@article{simonsohn2014,
	title = {P-curve: {A} key to the file-drawer},
	volume = {143},
	issn = {1939-2222},
	shorttitle = {P-curve},
	doi = {10.1037/a0033242},
	abstract = {Because scientists tend to report only studies (publication bias) or analyses (p-hacking) that “work,” readers must ask, “Are these effects true, or do they merely reflect selective reporting?” We introduce p-curve as a way to answer this question. P-curve is the distribution of statistically significant p values for a set of studies (ps p-curves—containing more low (.01s) than high (.04s) significant p values—only right-skewed p-curves are diagnostic of evidential value. By telling us whether we can rule out selective reporting as the sole explanation for a set of findings, p-curve offers a solution to the age-old inferential problems caused by file-drawers of failed studies and analyses. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {2},
	journal = {Journal of Experimental Psychology: General},
	author = {Simonsohn, Uri and Nelson, Leif D. and Simmons, Joseph P.},
	year = {2014},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Hypothesis Testing, Psychology, Publication Bias, Scientific Communication, Statistics},
	pages = {534--547},
}

@article{kerr1998,
	title = {{HARKing}: {Hypothesizing} {After} the {Results} are {Known}},
	volume = {2},
	issn = {1088-8683},
	shorttitle = {{HARKing}},
	url = {https://doi.org/10.1207/s15327957pspr0203_4},
	doi = {10.1207/s15327957pspr0203_4},
	abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as if it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing's costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
	language = {en},
	number = {3},
	urldate = {2024-10-09},
	journal = {Personality and Social Psychology Review},
	author = {Kerr, Norbert L.},
	month = aug,
	year = {1998},
	note = {Publisher: SAGE Publications Inc},
	pages = {196--217},
	file = {Submitted Version:/Users/joelpick/Zotero/storage/CRQJ4TVL/Kerr - 1998 - HARKing Hypothesizing After the Results are Known.pdf:application/pdf},
}

@article{ioannidis2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	issn = {1549-1676},
	url = {https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Summary There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {en},
	number = {8},
	urldate = {2024-10-09},
	journal = {PLOS Medicine},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	note = {Publisher: Public Library of Science},
	keywords = {Cancer risk factors, Finance, Genetic epidemiology, Genetics of disease, Metaanalysis, Randomized controlled trials, Research design, Schizophrenia},
	pages = {e124},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/RATQBFXT/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf},
}

@article{forstmeier2017a,
	title = {Detecting and avoiding likely false-positive findings – a practical guide},
	volume = {92},
	copyright = {© 2016 The Authors. Biological Reviews published by John Wiley \& Sons Ltd on behalf of Cambridge Philosophical Society.},
	issn = {1469-185X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12315},
	doi = {10.1111/brv.12315},
	abstract = {Recently there has been a growing concern that many published research findings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of ‘you can publish if you found a significant effect’. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difficult to falsify. In order to pinpoint the sources of error and possible solutions, we review current scientific practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive findings is expected to increase with (i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher flexibility, and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points (clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of ‘you can publish if your study is rigorous’. To this end, we highlight promising strategies towards making science more objective. Specifically, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis, and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research findings of one's own and of others for the benefit of the scientific community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of ‘impact’ almost exclusively and towards a system which explicitly values indices of scientific rigour.},
	language = {en},
	number = {4},
	urldate = {2024-10-09},
	journal = {Biological Reviews},
	author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, Timothy H.},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12315},
	keywords = {confirmation bias, HARKing, hindsight bias, overfitting, P-hacking, power, preregistration, replication, researcher degrees of freedom, Type I error},
	pages = {1941--1968},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/LC6L6AJR/Forstmeier et al. - 2017 - Detecting and avoiding likely false-positive findings – a practical guide.pdf:application/pdf;Snapshot:/Users/joelpick/Zotero/storage/PCE2MNMK/brv.html:text/html},
}

@article{sterne2000,
	title = {Publication and related bias in meta-analysis: {Power} of statistical tests and prevalence in the literature},
	volume = {53},
	issn = {0895-4356, 1878-5921},
	shorttitle = {Publication and related bias in meta-analysis},
	url = {https://www.jclinepi.com/article/S0895-4356(00)00242-0/abstract},
	doi = {10.1016/S0895-4356(00)00242-0},
	language = {English},
	number = {11},
	urldate = {2024-10-09},
	journal = {Journal of Clinical Epidemiology},
	author = {Sterne, Jonathan A. C. and Gavaghan, David and Egger, Matthias},
	month = nov,
	year = {2000},
	pmid = {11106885},
	note = {Publisher: Elsevier},
	keywords = {Correlation, Funnel plot, Meta-analysis, Publication bias, Regression, Simulation study},
	pages = {1119--1129},
}

@article{lakens2024,
	title = {When and {How} to {Deviate} {From} a {Preregistration}},
	volume = {10},
	issn = {2474-7394},
	url = {https://doi.org/10.1525/collabra.117094},
	doi = {10.1525/collabra.117094},
	abstract = {As the practice of preregistration becomes more common, researchers need guidance in how to report deviations from their preregistered statistical analysis plan. A principled approach to the use of preregistration should not treat all deviations as problematic. Deviations from a preregistered analysis plan can both reduce and increase the severity of a test, as well as increase the validity of inferences. I provide examples of how researchers can present deviations from preregistrations and evaluate the consequences of the deviation when encountering 1) unforeseen events, 2) errors in the preregistration, 3) missing information, 4) violations of untested assumptions, and 5) falsification of auxiliary hypotheses. The current manuscript aims to provide a principled approach to deciding when to deviate from a preregistration and how to report deviations from an error-statistical philosophy grounded in methodological falsificationism. The goal is to help researchers reflect on the consequence of deviations from preregistrations by evaluating the test’s severity and the validity of the inference.},
	number = {1},
	urldate = {2024-10-09},
	journal = {Collabra: Psychology},
	author = {Lakens, Daniël},
	month = may,
	year = {2024},
	pages = {117094},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/H6FENPPH/Lakens - 2024 - When and How to Deviate From a Preregistration.pdf:application/pdf;Snapshot:/Users/joelpick/Zotero/storage/39BBE72Q/When-and-How-to-Deviate-From-a-Preregistration.html:text/html},
}

@article{wagenmakers2012,
	title = {An {Agenda} for {Purely} {Confirmatory} {Research}},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612463078},
	doi = {10.1177/1745691612463078},
	abstract = {The veracity of substantive research claims hinges on the way experimental data are collected and analyzed. In this article, we discuss an uncomfortable fact that threatens the core of psychology’s academic enterprise: almost without exception, psychologists do not commit themselves to a method of data analysis before they see the actual data. It then becomes tempting to fine tune the analysis to the data in order to obtain a desired result—a procedure that invalidates the interpretation of the common statistical tests. The extent of the fine tuning varies widely across experiments and experimenters but is almost impossible for reviewers and readers to gauge. To remedy the situation, we propose that researchers preregister their studies and indicate in advance the analyses they intend to conduct. Only these analyses deserve the label “confirmatory,” and only for these analyses are the common statistical tests valid. Other analyses can be carried out but these should be labeled “exploratory.” We illustrate our proposal with a confirmatory replication attempt of a study on extrasensory perception.},
	language = {en},
	number = {6},
	urldate = {2024-10-09},
	journal = {Perspectives on Psychological Science},
	author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and van der Maas, Han L. J. and Kievit, Rogier A.},
	month = nov,
	year = {2012},
	note = {Publisher: SAGE Publications Inc},
	pages = {632--638},
	file = {SAGE PDF Full Text:/Users/joelpick/Zotero/storage/NBH7EM7C/Wagenmakers et al. - 2012 - An Agenda for Purely Confirmatory Research.pdf:application/pdf},
}

@article{chambers2022,
	title = {The past, present and future of {Registered} {Reports}},
	volume = {6},
	copyright = {2021 Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01193-7},
	doi = {10.1038/s41562-021-01193-7},
	abstract = {Registered Reports are a form of empirical publication in which study proposals are peer reviewed and pre-accepted before research is undertaken. By deciding which articles are published based on the question, theory and methods, Registered Reports offer a remedy for a range of reporting and publication biases. Here, we reflect on the history, progress and future prospects of the Registered Reports initiative and offer practical guidance for authors, reviewers and editors. We review early evidence that Registered Reports are working as intended, while at the same time acknowledging that they are not a universal solution for irreproducibility. We also consider how the policies and practices surrounding Registered Reports are changing, or must change in the future, to address limitations and adapt to new challenges. We conclude that Registered Reports are promoting reproducibility, transparency and self-correction across disciplines and may help reshape how society evaluates research and researchers.},
	language = {en},
	number = {1},
	urldate = {2024-10-09},
	journal = {Nature Human Behaviour},
	author = {Chambers, Christopher D. and Tzavella, Loukia},
	month = jan,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Culture, Publishing},
	pages = {29--42},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/K7Z8BSJJ/Chambers and Tzavella - 2022 - The past, present and future of Registered Reports.pdf:application/pdf},
}

@article{willroth2024,
	title = {Best {Laid} {Plans}: {A} {Guide} to {Reporting} {Preregistration} {Deviations}},
	volume = {7},
	issn = {2515-2459},
	shorttitle = {Best {Laid} {Plans}},
	url = {https://doi.org/10.1177/25152459231213802},
	doi = {10.1177/25152459231213802},
	abstract = {Psychological scientists are increasingly using preregistration as a tool to increase the credibility of research findings. Many of the benefits of preregistration rest on the assumption that preregistered plans are followed perfectly. However, research suggests that this is the exception rather than the norm, and there are many reasons why researchers may deviate from their preregistered plans. Preregistration can still be a valuable tool, even in the presence of deviations, as long as those deviations are well documented and transparently reported. Unfortunately, most preregistration deviations in psychology go unreported or are reported in unsystematic ways. In the current article, we offer a solution to this problem by providing a framework for transparent and standardized reporting of preregistration deviations, which was developed by drawing on our own experiences with preregistration, existing unpublished templates, feedback from colleagues and reviewers, and the results of a survey of 34 psychology-journal editors. This framework provides a clear template for what to do when things do not go as planned. We conclude by encouraging researchers to adopt this framework in their own preregistered research and by suggesting that journals implement structural policies around the transparent reporting of preregistration deviations.},
	language = {en},
	number = {1},
	urldate = {2024-10-09},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Willroth, Emily C. and Atherton, Olivia E.},
	month = jan,
	year = {2024},
	note = {Publisher: SAGE Publications Inc},
	pages = {25152459231213802},
	file = {SAGE PDF Full Text:/Users/joelpick/Zotero/storage/4ANCG2MV/Willroth and Atherton - 2024 - Best Laid Plans A Guide to Reporting Preregistration Deviations.pdf:application/pdf},
}

@article{purgar2024,
	title = {Supporting study registration to reduce research waste},
	volume = {8},
	copyright = {2024 Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-024-02433-5},
	doi = {10.1038/s41559-024-02433-5},
	abstract = {An estimated 82–89\% of ecological research and 85\% of medical research has limited or no value to the end user because of various inefficiencies. We argue that registration and registered reports can enhance the quality and impact of ecological research. Drawing on evidence from other fields, chiefly medicine, we support our claim that registration can reduce research waste. However, increasing registration rates, quality and impact will be very slow without coordinated effort of funders, publishers and research institutions. We therefore call on them to facilitate the adoption of registration by providing adequate support. We outline several aspects to be considered when designing a registration system that would best serve the field of ecology. To further inform the development of such a system, we call for more research to identify the causes of low registration rates in ecology. We suggest short- and long-term actions to bolster registration and reduce research waste.},
	language = {en},
	number = {8},
	urldate = {2024-10-09},
	journal = {Nature Ecology \& Evolution},
	author = {Purgar, Marija and Glasziou, Paul and Klanjscek, Tin and Nakagawa, Shinichi and Culina, Antica},
	month = aug,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Ecology, Scientific community},
	pages = {1391--1399},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/JHV9MXTA/Purgar et al. - 2024 - Supporting study registration to reduce research waste.pdf:application/pdf},
}

@article{schafer2019,
	title = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}: {Differences} {Between} {Sub}-{Disciplines} and the {Impact} of {Potential} {Biases}},
	volume = {10},
	issn = {1664-1078},
	shorttitle = {The {Meaningfulness} of {Effect} {Sizes} in {Psychological} {Research}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.00813/full},
	doi = {10.3389/fpsyg.2019.00813},
	abstract = {{\textless}p{\textgreater}Effect sizes are the currency of psychological research. They quantify the results of a study to answer the research question and are used to calculate statistical power. The interpretation of effect sizes—when is an effect small, medium, or large?—has been guided by the recommendations Jacob Cohen gave in his pioneering writings starting in 1962: Either compare an effect with the effects found in past research or use certain conventional benchmarks. The present analysis shows that neither of these recommendations is currently applicable. From past publications without pre-registration, 900 effects were randomly drawn and compared with 93 effects from publications with pre-registration, revealing a large difference: Effects from the former (median {\textless}italic{\textgreater}r{\textless}/italic{\textgreater} = 0.36) were much larger than effects from the latter (median {\textless}italic{\textgreater}r{\textless}/italic{\textgreater} = 0.16). That is, certain biases, such as publication bias or questionable research practices, have caused a dramatic inflation in published effects, making it difficult to compare an actual effect with the real population effects (as these are unknown). In addition, there were very large differences in the mean effects between psychological sub-disciplines and between different study designs, making it impossible to apply any global benchmarks. Many more pre-registered studies are needed in the future to derive a reliable picture of real population effects.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-10-09},
	journal = {Frontiers in Psychology},
	author = {Schäfer, Thomas and Schwarz, Marcus A.},
	month = apr,
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {Cohen, effect size, power, Publication Bias, replicability, Replication, Sample Size},
	file = {Full Text:/Users/joelpick/Zotero/storage/9EKRINWN/Schäfer and Schwarz - 2019 - The Meaningfulness of Effect Sizes in Psychological Research Differences Between Sub-Disciplines an.pdf:application/pdf},
}

@article{vandenakker2024,
	title = {Preregistration in practice: {A} comparison of preregistered and non-preregistered studies in psychology},
	volume = {56},
	issn = {1554-3528},
	shorttitle = {Preregistration in practice},
	url = {https://doi.org/10.3758/s13428-023-02277-0},
	doi = {10.3758/s13428-023-02277-0},
	abstract = {Preregistration has gained traction as one of the most promising solutions to improve the replicability of scientific effects. In this project, we compared 193 psychology studies that earned a Preregistration Challenge prize or preregistration badge to 193 related studies that were not preregistered. In contrast to our theoretical expectations and prior research, we did not find that preregistered studies had a lower proportion of positive results (Hypothesis 1), smaller effect sizes (Hypothesis 2), or fewer statistical errors (Hypothesis 3) than non-preregistered studies. Supporting our Hypotheses 4 and 5, we found that preregistered studies more often contained power analyses and typically had larger sample sizes than non-preregistered studies. Finally, concerns about the publishability and impact of preregistered studies seem unwarranted, as preregistered studies did not take longer to publish and scored better on several impact measures. Overall, our data indicate that preregistration has beneficial effects in the realm of statistical power and impact, but we did not find robust evidence that preregistration prevents p-hacking and HARKing (Hypothesizing After the Results are Known).},
	language = {en},
	number = {6},
	urldate = {2024-10-09},
	journal = {Behavior Research Methods},
	author = {van den Akker, Olmo R. and van Assen, Marcel A. L. M. and Bakker, Marjan and Elsherif, Mahmoud and Wong, Tsz Keung and Wicherts, Jelte M.},
	month = sep,
	year = {2024},
	keywords = {Effect size, HARKing, P-hacking, Positive results, Preregistration, Research impact},
	pages = {5424--5433},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/YHPC8UVB/van den Akker et al. - 2024 - Preregistration in practice A comparison of preregistered and non-preregistered studies in psycholo.pdf:application/pdf},
}

@article{toth2021,
	title = {Study {Preregistration}: {An} {Evaluation} of a {Method} for {Transparent} {Reporting}},
	volume = {36},
	issn = {1573-353X},
	shorttitle = {Study {Preregistration}},
	url = {https://doi.org/10.1007/s10869-020-09695-3},
	doi = {10.1007/s10869-020-09695-3},
	abstract = {Study preregistration promotes transparency in scientific research by making a clear distinction between a priori and post hoc procedures or analyses. Management and applied psychology have not embraced preregistration in the way other closely related social science fields have. There may be concerns that preregistration does not add value and prevents exploratory data analyses. Using a mixed-method approach, in Study 1, we compared published preregistered samples against published non-preregistered samples. We found that preregistration effectively facilitated more transparent reporting based on criteria (i.e., confirmed hypotheses and a priori analysis plans). Moreover, consistent with concerns that the published literature contains elevated type I error rates, preregistered samples had fewer statistically significant results (48\%) than non-preregistered samples (66\%). To learn about the perceived advantages, disadvantages, and misconceptions of study preregistration, in Study 2, we surveyed authors of preregistered studies and authors who had never preregistered a study. Participants in both samples had positive inclinations towards preregistration yet expressed concerns about the process. We conclude with a review of best practices for management and applied psychology stakeholders.},
	language = {en},
	number = {4},
	urldate = {2024-10-09},
	journal = {Journal of Business and Psychology},
	author = {Toth, Allison A. and Banks, George C. and Mellor, David and O’Boyle, Ernest H. and Dickson, Ashleigh and Davis, Daniel J. and DeHaven, Alex and Bochantin, Jaime and Borns, Jared},
	month = aug,
	year = {2021},
	keywords = {Artificial Intelligence, Medical Ethics, Methodology, Open science, Preregistration, Questionable research practices, Reproducibility},
	pages = {553--571},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/TR3SH6V5/Toth et al. - 2021 - Study Preregistration An Evaluation of a Method for Transparent Reporting.pdf:application/pdf},
}

@article{heirene2024,
	title = {Preregistration specificity and adherence: {A} review of preregistered gambling studies and cross-disciplinary comparison},
	volume = {8},
	copyright = {Copyright (c) 2024 Robert Heirene, Debi LaPlante, Eric Louderback, Brittany Keen, Marjan Bakker, Anastasia Serafimovska, Sally Gainsbury},
	issn = {2003-2714},
	shorttitle = {Preregistration specificity and adherence},
	url = {https://open.lnu.se/index.php/metapsychology/article/view/2909},
	doi = {10.15626/MP.2021.2909},
	abstract = {Study preregistration is one of several “open science” practices (e.g., open data, preprints) that researchers use to improve the transparency and rigour of their research. As more researchers adopt preregistration as a regular practice, examining the nature and content of preregistrations can help identify the strengths and weaknesses of current practices. The value of preregistration, in part, relates to the specificity of the study plan and the extent to which investigators adhere to this plan. We identified 53 preregistrations from the gambling studies field meeting our predefined eligibility criteria and scored their level of specificity using a 23-item protocol developed to measure the extent to which a clear and exhaustive preregistration plan restricts various researcher degrees of freedom (RDoF; i.e., the many methodological choices available to researchers when collecting and analysing data, and when reporting their findings). We also scored studies on a 32-item protocol that measured adherence to the preregistered plan in the study manuscript. We found gambling preregistrations had low specificity levels on most RDoF. However, a comparison with a sample of cross-disciplinary preregistrations (N = 52; Bakker et al., 2020) indicated that gambling preregistrations scored higher on 12 (of 29) items. Thirteen (65\%) of the 20 associated published articles or preprints deviated from the protocol without declaring as much (the mean number of undeclared deviations per article was 2.25, SD = 2.34). Overall, while we found improvements in specificity and adherence over time (2017-2020), our findings suggest the purported benefits of preregistration—including increasing transparency and reducing RDoF—are not fully achieved by current practices. Using our findings, we provide 10 practical recommendations that can be used to support and refine preregistration practices.},
	language = {en},
	urldate = {2024-10-09},
	journal = {Meta-Psychology},
	author = {Heirene, Robert and LaPlante, Debi and Louderback, Eric and Keen, Brittany and Bakker, Marjan and Serafimovska, Anastasia and Gainsbury, Sally},
	month = jul,
	year = {2024},
	keywords = {Addiction, Gambling, Meta-science, Open Science, Preregistration, Researcher degrees of freedom},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/XQB24RDA/Heirene et al. - 2024 - Preregistration specificity and adherence A review of preregistered gambling studies and cross-disc.pdf:application/pdf},
}

@article{brodeur2024,
	title = {Do {Preregistration} and {Preanalysis} {Plans} {Reduce} p-{Hacking} and {Publication} {Bias}? {Evidence} from 15,992 {Test} {Statistics} and {Suggestions} for {Improvement}},
	volume = {2},
	issn = {2832-9368},
	shorttitle = {Do {Preregistration} and {Preanalysis} {Plans} {Reduce} p-{Hacking} and {Publication} {Bias}?},
	url = {https://www.journals.uchicago.edu/doi/full/10.1086/730455},
	doi = {10.1086/730455},
	abstract = {Preregistration is regarded as an important contributor to research credibility. We investigate this by analyzing the pattern of test statistics from the universe of randomized controlled trial studies published in 15 leading economics journals. We draw two conclusions: (a) Preregistration frequently does not involve a preanalysis plan (PAP), or sufficient detail to constrain meaningfully the actions and decisions of researchers after data are collected. Consistent with this, we find no evidence that preregistration in itself reduces p-hacking and publication bias. (b) When preregistration is accompanied by a PAP we find evidence consistent with both reduced p-hacking and reduced publication bias.},
	number = {3},
	urldate = {2024-10-09},
	journal = {Journal of Political Economy Microeconomics},
	author = {Brodeur, Abel and Cook, Nikolai M. and Hartley, Jonathan S. and Heyes, Anthony},
	month = aug,
	year = {2024},
	note = {Publisher: The University of Chicago Press},
	pages = {527--561},
}

@article{vandenakker2023,
	title = {Selective {Hypothesis} {Reporting} in {Psychology}: {Comparing} {Preregistrations} and {Corresponding} {Publications}},
	volume = {6},
	issn = {2515-2459},
	shorttitle = {Selective {Hypothesis} {Reporting} in {Psychology}},
	url = {https://doi.org/10.1177/25152459231187988},
	doi = {10.1177/25152459231187988},
	abstract = {In this study, we assessed the extent of selective hypothesis reporting in psychological research by comparing the hypotheses found in a set of 459 preregistrations with the hypotheses found in the corresponding articles. We found that more than half of the preregistered studies we assessed contained omitted hypotheses (N = 224; 52\%) or added hypotheses (N = 227; 57\%), and about one-fifth of studies contained hypotheses with a direction change (N = 79; 18\%). We found only a small number of studies with hypotheses that were demoted from primary to secondary importance (N = 2; 1\%) and no studies with hypotheses that were promoted from secondary to primary importance. In all, 60\% of studies included at least one hypothesis in one or more of these categories, indicating a substantial bias in presenting and selecting hypotheses by researchers and/or reviewers/editors. Contrary to our expectations, we did not find sufficient evidence that added hypotheses and changed hypotheses were more likely to be statistically significant than nonselectively reported hypotheses. For the other types of selective hypothesis reporting, we likely did not have sufficient statistical power to test for a relationship with statistical significance. Finally, we found that replication studies were less likely to include selectively reported hypotheses than original studies. In all, selective hypothesis reporting is problematically common in psychological research. We urge researchers, reviewers, and editors to ensure that hypotheses outlined in preregistrations are clearly formulated and accurately presented in the corresponding articles.},
	language = {en},
	number = {3},
	urldate = {2024-10-09},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {van den Akker, Olmo R. and van Assen, Marcel A. L. M. and Enting, Manon and de Jonge, Myrthe and Ong, How Hwee and Rüffer, Franziska and Schoenmakers, Martijn and Stoevenbelt, Andrea H. and Wicherts, Jelte M. and Bakker, Marjan},
	month = jul,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
	pages = {25152459231187988},
	file = {SAGE PDF Full Text:/Users/joelpick/Zotero/storage/S5UR7V2H/van den Akker et al. - 2023 - Selective Hypothesis Reporting in Psychology Comparing Preregistrations and Corresponding Publicati.pdf:application/pdf},
}

@article{claesen2021,
	title = {Comparing dream to reality: an assessment of adherence of the first generation of preregistered studies},
	volume = {8},
	shorttitle = {Comparing dream to reality},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.211037},
	doi = {10.1098/rsos.211037},
	abstract = {Preregistration is a method to increase research transparency by documenting research decisions on a public, third-party repository prior to any influence by data. It is becoming increasingly popular in all subfields of psychology and beyond. Adherence to the preregistration plan may not always be feasible and even is not necessarily desirable, but without disclosure of deviations, readers who do not carefully consult the preregistration plan might get the incorrect impression that the study was exactly conducted and reported as planned. In this paper, we have investigated adherence and disclosure of deviations for all articles published with the Preregistered badge in Psychological Science between February 2015 and November 2017 and shared our findings with the corresponding authors for feedback. Two out of 27 preregistered studies contained no deviations from the preregistration plan. In one study, all deviations were disclosed. Nine studies disclosed none of the deviations. We mainly observed (un)disclosed deviations from the plan regarding the reported sample size, exclusion criteria and statistical analysis. This closer look at preregistrations of the first generation reveals possible hurdles for reporting preregistered studies and provides input for future reporting guidelines. We discuss the results and possible explanations, and provide recommendations for preregistered research.},
	number = {10},
	urldate = {2024-10-09},
	journal = {Royal Society Open Science},
	author = {Claesen, Aline and Gomes, Sara and Tuerlinckx, Francis and Vanpaemel, Wolf},
	month = oct,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {open science, preregistration, psychological science, researcher degrees of freedom, transparency},
	pages = {211037},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/DT5SULYP/Claesen et al. - 2021 - Comparing dream to reality an assessment of adherence of the first generation of preregistered stud.pdf:application/pdf},
}

@misc{vandenakker2023a,
	title = {The effectiveness of preregistration in psychology: {Assessing} preregistration strictness and preregistration-study consistency},
	shorttitle = {The effectiveness of preregistration in psychology},
	url = {https://osf.io/h8xjw},
	doi = {10.31222/osf.io/h8xjw},
	abstract = {Study preregistration has become increasingly popular in psychology, but its potential to restrict researcher degrees of freedom has not yet been empirically verified. We used an extensive protocol to assess the producibility (i.e., the degree to which a study can be properly conducted based on the available information) of preregistrations and the consistency between preregistrations and their corresponding papers for 300 psychology studies. We found that preregistrations often lack methodological details and that undisclosed deviations from preregistered plans are frequent. These results highlight that biases due to researcher degrees of freedom remain possible in many preregistered studies. More comprehensive registration templates typically yielded more producible preregistrations. We did not find that the producibility and consistency of preregistrations differed over time or between original and replication studies. Furthermore, we found that operationalizations of variables were generally preregistered more producible and consistently than other study parts. Inconsistencies between preregistrations and published studies were mainly encountered for data collection procedures, statistical models, and exclusion criteria. Our results indicate that, to unlock the full potential of preregistration, researchers in psychology should aim to write more producible preregistrations, adhere to these preregistrations more faithfully, and more transparently report any deviations from their preregistrations. This could be facilitated by training and education to improve preregistration skills, as well as the development of more comprehensive templates.},
	language = {en-us},
	urldate = {2024-10-09},
	publisher = {OSF},
	author = {van den Akker, Olmo R. and Bakker, Marjan and van Assen, Marcel A. L. M. and Pennington, Charlotte Rebecca and Verweij, Leone and Elsherif, Mahmoud and Claesen, Aline and Gaillard, Stefan Daniel Michel and Yeung, Siu Kit and Frankenberger, Jan-Luca and Krautter, Kai and Cockcroft, Jamie P. and Kreuer, Katharina Sybille and Evans, Thomas Rhys and Heppel, Frédérique and Schoch, Sarah Fiona and Korbmacher, Max and Yamada, Yuki and Albayrak-Aydemir, Nihan and Alzahawi, Shilaan and Sarafoglou, Alexandra and Sitnikov, Maksim and Dechterenko, Filip and Wingen, Sophia and Grinschgl, Sandra and Hartmann, Helena and Stewart, Suzanne L. K. and Oliveira, Catia Margarida and Ashcroft-Jones, Sarah and Baker, Bradley James and Wicherts, Jelte},
	month = may,
	year = {2023},
	keywords = {preregistration},
}

@article{sarafoglou2022,
	title = {A survey on how preregistration affects the research workflow: better science but more work},
	volume = {9},
	shorttitle = {A survey on how preregistration affects the research workflow},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.211997},
	doi = {10.1098/rsos.211997},
	abstract = {The preregistration of research protocols and analysis plans is a main reform innovation to counteract confirmation bias in the social and behavioural sciences. While theoretical reasons to preregister are frequently discussed in the literature, the individually experienced advantages and disadvantages of this method remain largely unexplored. The goal of this exploratory study was to identify the perceived benefits and challenges of preregistration from the researcher’s perspective. To this end, we surveyed 355 researchers, 299 of whom had used preregistration in their own work. The researchers indicated the experienced or expected effects of preregistration on their workflow. The results show that experiences and expectations are mostly positive. Researchers in our sample believe that implementing preregistration improves or is likely to improve the quality of their projects. Criticism of preregistration is primarily related to the increase in work-related stress and the overall duration of the project. While the benefits outweighed the challenges for the majority of researchers with preregistration experience, this was not the case for the majority of researchers without preregistration experience. The experienced advantages and disadvantages identified in our survey could inform future efforts to improve preregistration and thus help the methodology gain greater acceptance in the scientific community.},
	number = {7},
	urldate = {2024-10-09},
	journal = {Royal Society Open Science},
	author = {Sarafoglou, Alexandra and Kovacs, Marton and Bakos, Bence and Wagenmakers, Eric-Jan and Aczel, Balazs},
	month = jul,
	year = {2022},
	note = {Publisher: Royal Society},
	keywords = {meta-science, open science, replication crisis},
	pages = {211997},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/AISLK86I/Sarafoglou et al. - 2022 - A survey on how preregistration affects the research workflow better science but more work.pdf:application/pdf},
}

@article{scheel2021,
	title = {An {Excess} of {Positive} {Results}: {Comparing} the {Standard} {Psychology} {Literature} {With} {Registered} {Reports}},
	volume = {4},
	issn = {2515-2459},
	shorttitle = {An {Excess} of {Positive} {Results}},
	url = {https://doi.org/10.1177/25152459211007467},
	doi = {10.1177/25152459211007467},
	abstract = {Selectively publishing results that support the tested hypotheses (“positive” results) distorts the available evidence for scientific claims. For the past decade, psychological scientists have been increasingly concerned about the degree of such distortion in their literature. A new publication format has been developed to prevent selective reporting: In Registered Reports (RRs), peer review and the decision to publish take place before results are known. We compared the results in published RRs (N = 71 as of November 2018) with a random sample of hypothesis-testing studies from the standard literature (N = 152) in psychology. Analyzing the first hypothesis of each article, we found 96\% positive results in standard reports but only 44\% positive results in RRs. We discuss possible explanations for this large difference and suggest that a plausible factor is the reduction of publication bias and/or Type I error inflation in the RR literature.},
	language = {en},
	number = {2},
	urldate = {2024-10-09},
	journal = {Advances in Methods and Practices in Psychological Science},
	author = {Scheel, Anne M. and Schijen, Mitchell R. M. J. and Lakens, Daniël},
	month = apr,
	year = {2021},
	note = {Publisher: SAGE Publications Inc},
	pages = {25152459211007467},
	file = {SAGE PDF Full Text:/Users/joelpick/Zotero/storage/UEILFP4U/Scheel et al. - 2021 - An Excess of Positive Results Comparing the Standard Psychology Literature With Registered Reports.pdf:application/pdf},
}

@article{allen2019,
	title = {Open science challenges, benefits and tips in early career and beyond},
	volume = {17},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000246},
	doi = {10.1371/journal.pbio.3000246},
	abstract = {The movement towards open science is a consequence of seemingly pervasive failures to replicate previous research. This transition comes with great benefits but also significant challenges that are likely to affect those who carry out the research, usually early career researchers (ECRs). Here, we describe key benefits, including reputational gains, increased chances of publication, and a broader increase in the reliability of research. The increased chances of publication are supported by exploratory analyses indicating null findings are substantially more likely to be published via open registered reports in comparison to more conventional methods. These benefits are balanced by challenges that we have encountered and that involve increased costs in terms of flexibility, time, and issues with the current incentive structure, all of which seem to affect ECRs acutely. Although there are major obstacles to the early adoption of open science, overall open science practices should benefit both the ECR and improve the quality of research. We review 3 benefits and 3 challenges and provide suggestions from the perspective of ECRs for moving towards open science practices, which we believe scientists and institutions at all levels would do well to consider.},
	language = {en},
	number = {5},
	urldate = {2024-10-09},
	journal = {PLOS Biology},
	author = {Allen, Christopher and Mehler, David M. A.},
	month = may,
	year = {2019},
	note = {Publisher: Public Library of Science},
	keywords = {Careers, Experimental design, Neuroimaging, Open data, Open science, Peer review, Reproducibility, Statistical data},
	pages = {e3000246},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/E3BZIVH5/Allen and Mehler - 2019 - Open science challenges, benefits and tips in early career and beyond.pdf:application/pdf},
}

@article{soderberg2021,
	title = {Initial evidence of research quality of registered reports compared with the standard publishing model},
	volume = {5},
	copyright = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01142-4},
	doi = {10.1038/s41562-021-01142-4},
	abstract = {In registered reports (RRs), initial peer review and in-principle acceptance occur before knowing the research outcomes. This combats publication bias and distinguishes planned from unplanned research. How RRs could improve the credibility of research findings is straightforward, but there is little empirical evidence. Also, there could be unintended costs such as reducing novelty. Here, 353 researchers peer reviewed a pair of papers from 29 published RRs from psychology and neuroscience and 57 non-RR comparison papers. RRs numerically outperformed comparison papers on all 19 criteria (mean difference 0.46, scale range −4 to +4) with effects ranging from RRs being statistically indistinguishable from comparison papers in novelty (0.13, 95\% credible interval [−0.24, 0.49]) and creativity (0.22, [−0.14, 0.58]) to sizeable improvements in rigour of methodology (0.99, [0.62, 1.35]) and analysis (0.97, [0.60, 1.34]) and overall paper quality (0.66, [0.30, 1.02]). RRs could improve research quality while reducing publication bias and ultimately improve the credibility of the published literature.},
	language = {en},
	number = {8},
	urldate = {2024-10-09},
	journal = {Nature Human Behaviour},
	author = {Soderberg, Courtney K. and Errington, Timothy M. and Schiavone, Sarah R. and Bottesini, Julia and Thorn, Felix Singleton and Vazire, Simine and Esterling, Kevin M. and Nosek, Brian A.},
	month = aug,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Psychology, Publishing},
	pages = {990--997},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/28MP6DYX/Soderberg et al. - 2021 - Initial evidence of research quality of registered reports compared with the standard publishing mod.pdf:application/pdf},
}

@article{costa2022,
	title = {Do {Registered} {Reports} {Make} {Scientific} {Findings} {More} {Believable} to the {Public}?},
	volume = {8},
	issn = {2474-7394},
	url = {https://doi.org/10.1525/collabra.32607},
	doi = {10.1525/collabra.32607},
	abstract = {Registered reports are an important initiative to improve the methodological rigor and transparency of scientific studies. One possible benefit of registered reports is that they may increase public acceptance of controversial research findings. We test this question by providing participants in a large US-based sample (n = 1,500) with descriptions of the key features of registered reports and the standard peer-review process, and then eliciting credibility judgments for various scientific results. We do not find evidence that participants view findings from registered reports as more credible than findings conducted under a standard (non-registered) report. This was true for both plausible and implausible study findings. Our results help clarify public attitudes and beliefs about scientific findings in light of recent methodological developments.},
	number = {1},
	urldate = {2024-10-09},
	journal = {Collabra: Psychology},
	author = {Costa, Elaine and Inbar, Yoel and Tannenbaum, David},
	editor = {Holcombe, Alex},
	month = mar,
	year = {2022},
	pages = {32607},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/XCN8HIGW/Costa et al. - 2022 - Do Registered Reports Make Scientific Findings More Believable to the Public.pdf:application/pdf;Snapshot:/Users/joelpick/Zotero/storage/DH5FT6LP/Do-Registered-Reports-Make-Scientific-Findings.html:text/html},
}

@article{fanelli2012,
	title = {Negative results are disappearing from most disciplines and countries},
	volume = {90},
	issn = {1588-2861},
	url = {https://doi.org/10.1007/s11192-011-0494-7},
	doi = {10.1007/s11192-011-0494-7},
	abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have “tested” a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
	language = {en},
	number = {3},
	urldate = {2024-10-09},
	journal = {Scientometrics},
	author = {Fanelli, Daniele},
	month = mar,
	year = {2012},
	keywords = {Bias, Competition, Misconduct, Publication, Publish or perish, Research evaluation},
	pages = {891--904},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/8ZGGPNHP/Fanelli - 2012 - Negative results are disappearing from most disciplines and countries.pdf:application/pdf},
}

@article{cassey2004,
	title = {A survey of publication bias within evolutionary ecology},
	volume = {271},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsbl.2004.0218},
	doi = {10.1098/rsbl.2004.0218},
	abstract = {Publication bias has been recognized as a problem in ecology and evolution that can undermine reviews of research results. Unfortunately, direct tests of publication bias are extremely rare. Here, we quantify a well–discussed but, to our knowledge, previously untested form of publication bias: the publication of results with and without estimates of effect size. We find that results published without effect sizes are a biased sample of those that are published. This further complicates the already difficult task of compiling quantitative literature reviews and meta–analytic studies.},
	number = {suppl\_6},
	urldate = {2024-10-09},
	journal = {Proceedings of the Royal Society of London. Series B: Biological Sciences},
	author = {Cassey, Phillip and Ewen, John G. and Blackburn, Tim M. and Møller, Anders P.},
	month = dec,
	year = {2004},
	note = {Publisher: Royal Society},
	keywords = {effect size, meta–analysis, reporting bias, within–study publication bias},
	pages = {S451--S454},
	file = {Full Text:/Users/joelpick/Zotero/storage/TBIFF7HC/Cassey et al. - 2004 - A survey of publication bias within evolutionary ecology.pdf:application/pdf},
}

@article{yang2023,
	title = {Publication bias impacts on effect size, statistical power, and magnitude ({Type} {M}) and sign ({Type} {S}) errors in ecology and evolutionary biology},
	volume = {21},
	issn = {1741-7007},
	url = {https://doi.org/10.1186/s12915-022-01485-y},
	doi = {10.1186/s12915-022-01485-y},
	abstract = {Collaborative efforts to directly replicate empirical studies in the medical and social sciences have revealed alarmingly low rates of replicability, a phenomenon dubbed the ‘replication crisis’. Poor replicability has spurred cultural changes targeted at improving reliability in these disciplines. Given the absence of equivalent replication projects in ecology and evolutionary biology, two inter-related indicators offer the opportunity to retrospectively assess replicability: publication bias and statistical power. This registered report assesses the prevalence and severity of small-study (i.e., smaller studies reporting larger effect sizes) and decline effects (i.e., effect sizes decreasing over time) across ecology and evolutionary biology using 87 meta-analyses comprising 4,250 primary studies and 17,638 effect sizes. Further, we estimate how publication bias might distort the estimation of effect sizes, statistical power, and errors in magnitude (Type M or exaggeration ratio) and sign (Type S). We show strong evidence for the pervasiveness of both small-study and decline effects in ecology and evolution. There was widespread prevalence of publication bias that resulted in meta-analytic means being over-estimated by (at least) 0.12 standard deviations. The prevalence of publication bias distorted confidence in meta-analytic results, with 66\% of initially statistically significant meta-analytic means becoming non-significant after correcting for publication bias. Ecological and evolutionary studies consistently had low statistical power (15\%) with a 4-fold exaggeration of effects on average (Type M error rates = 4.4). Notably, publication bias reduced power from 23\% to 15\% and increased type M error rates from 2.7 to 4.4 because it creates a non-random sample of effect size evidence. The sign errors of effect sizes (Type S error) increased from 5\% to 8\% because of publication bias. Our research provides clear evidence that many published ecological and evolutionary findings are inflated. Our results highlight the importance of designing high-power empirical studies (e.g., via collaborative team science), promoting and encouraging replication studies, testing and correcting for publication bias in meta-analyses, and adopting open and transparent research practices, such as (pre)registration, data- and code-sharing, and transparent reporting.},
	number = {1},
	urldate = {2024-10-09},
	journal = {BMC Biology},
	author = {Yang, Yefeng and Sánchez-Tójar, Alfredo and O’Dea, Rose E. and Noble, Daniel W. A. and Koricheva, Julia and Jennions, Michael D. and Parker, Timothy H. and Lagisz, Malgorzata and Nakagawa, Shinichi},
	month = apr,
	year = {2023},
	keywords = {Generalizability, Many labs, Meta-research, Open science, P-hacking, Questionable research practices, Registered report, Replicability, Reproducibility, Selective reporting, Transparency},
	pages = {71},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/659SI8ZX/Yang et al. - 2023 - Publication bias impacts on effect size, statistical power, and magnitude (Type M) and sign (Type S).pdf:application/pdf;Snapshot:/Users/joelpick/Zotero/storage/BISX2KQG/s12915-022-01485-y.html:text/html},
}

@article{kimmel2023,
	title = {Empirical evidence of widespread exaggeration bias and selective reporting in ecology},
	volume = {7},
	copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-023-02144-3},
	doi = {10.1038/s41559-023-02144-3},
	abstract = {In many scientific disciplines, common research practices have led to unreliable and exaggerated evidence about scientific phenomena. Here we describe some of these practices and quantify their pervasiveness in recent ecology publications in five popular journals. In an analysis of over 350 studies published between 2018 and 2020, we detect empirical evidence of exaggeration bias and selective reporting of statistically significant results. This evidence implies that the published effect sizes in ecology journals exaggerate the importance of the ecological relationships that they aim to quantify. An exaggerated evidence base hinders the ability of empirical ecology to reliably contribute to science, policy, and management. To increase the credibility of ecology research, we describe a set of actions that ecologists should take, including changes to scientific norms about what high-quality ecology looks like and expectations about what high-quality studies can deliver.},
	language = {en},
	number = {9},
	urldate = {2024-10-09},
	journal = {Nature Ecology \& Evolution},
	author = {Kimmel, Kaitlin and Avolio, Meghan L. and Ferraro, Paul J.},
	month = sep,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Ecology, Evolution},
	pages = {1525--1536},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/N7L3WLCW/Kimmel et al. - 2023 - Empirical evidence of widespread exaggeration bias and selective reporting in ecology.pdf:application/pdf},
}

@article{fidler2017,
	title = {Metaresearch for {Evaluating} {Reproducibility} in {Ecology} and {Evolution}},
	volume = {67},
	issn = {0006-3568},
	url = {https://doi.org/10.1093/biosci/biw159},
	doi = {10.1093/biosci/biw159},
	abstract = {Recent replication projects in other disciplines have uncovered disturbingly low levels of reproducibility, suggesting that those research literatures may contain unverifiable claims. The conditions contributing to irreproducibility in other disciplines are also present in ecology. These include a large discrepancy between the proportion of “positive” or “significant” results and the average statistical power of empirical research, incomplete reporting of sampling stopping rules and results, journal policies that discourage replication studies, and a prevailing publish-or-perish research culture that encourages questionable research practices. We argue that these conditions constitute sufficient reason to systematically evaluate the reproducibility of the evidence base in ecology and evolution. In some cases, the direct replication of ecological research is difficult because of strong temporal and spatial dependencies, so here, we propose metaresearch projects that will provide proxy measures of reproducibility.},
	number = {3},
	urldate = {2024-10-09},
	journal = {BioScience},
	author = {Fidler, Fiona and Chee, Yung En and Wintle, Bonnie C. and Burgman, Mark A. and McCarthy, Michael A. and Gordon, Ascelin},
	month = mar,
	year = {2017},
	pages = {282--289},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/IVA7RAEG/Fidler et al. - 2017 - Metaresearch for Evaluating Reproducibility in Ecology and Evolution.pdf:application/pdf;Snapshot:/Users/joelpick/Zotero/storage/8NT7W7TZ/2900173.html:text/html},
}

@article{fraser2018,
	title = {Questionable research practices in ecology and evolution},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303},
	doi = {10.1371/journal.pone.0200303},
	abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
	language = {en},
	number = {7},
	urldate = {2024-10-09},
	journal = {PLOS ONE},
	author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
	month = jul,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Behavioral ecology, Community ecology, Evolutionary biology, Evolutionary ecology, Evolutionary rate, Psychology, Publication ethics, Statistical data},
	pages = {e0200303},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/XRYUBAZM/Fraser et al. - 2018 - Questionable research practices in ecology and evolution.pdf:application/pdf},
}

@article{purgar2022,
	title = {Quantifying research waste in ecology},
	volume = {6},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-334X},
	url = {https://www.nature.com/articles/s41559-022-01820-0},
	doi = {10.1038/s41559-022-01820-0},
	abstract = {Research inefficiencies can generate huge waste: evidence from biomedical research has shown that most research is avoidably wasted and steps have been taken to tackle this costly problem. Although other scientific fields could also benefit from identifying and quantifying waste and acting to reduce it, no other estimates of research waste are available. Given that ecological issues interweave most of the United Nations Sustainable Development Goals, we argue that tackling research waste in ecology should be prioritized. Our study leads the way. We estimate components of waste in ecological research based on a literature review and a meta-analysis. Shockingly, our results suggest only 11–18\% of conducted ecological research reaches its full informative value. All actors within the research system—including academic institutions, policymakers, funders and publishers—have a duty towards science, the environment, study organisms and the public, to urgently act and reduce this considerable yet preventable loss. We discuss potential ways forward and call for two major actions: (1) further research into waste in ecology (and beyond); (2) focused development and implementation of solutions to reduce unused potential of ecological research.},
	language = {en},
	number = {9},
	urldate = {2024-10-09},
	journal = {Nature Ecology \& Evolution},
	author = {Purgar, Marija and Klanjscek, Tin and Culina, Antica},
	month = sep,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Ecology, Scientific community},
	pages = {1390--1397},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/5YAMKMCL/Purgar et al. - 2022 - Quantifying research waste in ecology.pdf:application/pdf},
}

@article{parker2016,
	title = {Promoting transparency in evolutionary biology and ecology},
	volume = {19},
	copyright = {© 2016 John Wiley \& Sons Ltd/CNRS},
	issn = {1461-0248},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.12610},
	doi = {10.1111/ele.12610},
	language = {en},
	number = {7},
	urldate = {2024-10-09},
	journal = {Ecology Letters},
	author = {Parker, T. H. and Nakagawa, S. and Gurevitch, J. and {IIEE (Improving Inference in Evolutionary Biology and Ecology) workshop participants}},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ele.12610},
	pages = {726--728},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/NKPAIFXI/Parker et al. - 2016 - Promoting transparency in evolutionary biology and ecology.pdf:application/pdf;Snapshot:/Users/joelpick/Zotero/storage/N5KG5CHZ/ele.html:text/html},
}

@article{hardwicke2020,
	title = {An empirical assessment of transparency and reproducibility-related research practices in the social sciences (2014–2017)},
	volume = {7},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.190806},
	doi = {10.1098/rsos.190806},
	abstract = {Serious concerns about research quality have catalysed a number of reform initiatives intended to improve transparency and reproducibility and thus facilitate self-correction, increase efficiency and enhance research credibility. Meta-research has evaluated the merits of some individual initiatives; however, this may not capture broader trends reflecting the cumulative contribution of these efforts. In this study, we manually examined a random sample of 250 articles in order to estimate the prevalence of a range of transparency and reproducibility-related indicators in the social sciences literature published between 2014 and 2017. Few articles indicated availability of materials (16/151, 11\% [95\% confidence interval, 7\% to 16\%]), protocols (0/156, 0\% [0\% to 1\%]), raw data (11/156, 7\% [2\% to 13\%]) or analysis scripts (2/156, 1\% [0\% to 3\%]), and no studies were pre-registered (0/156, 0\% [0\% to 1\%]). Some articles explicitly disclosed funding sources (or lack of; 74/236, 31\% [25\% to 37\%]) and some declared no conflicts of interest (36/236, 15\% [11\% to 20\%]). Replication studies were rare (2/156, 1\% [0\% to 3\%]). Few studies were included in evidence synthesis via systematic review (17/151, 11\% [7\% to 16\%]) or meta-analysis (2/151, 1\% [0\% to 3\%]). Less than half the articles were publicly available (101/250, 40\% [34\% to 47\%]). Minimal adoption of transparency and reproducibility-related research practices could be undermining the credibility and efficiency of social science research. The present study establishes a baseline that can be revisited in the future to assess progress.},
	number = {2},
	urldate = {2024-10-09},
	journal = {Royal Society Open Science},
	author = {Hardwicke, Tom E. and Wallach, Joshua D. and Kidwell, Mallory C. and Bendixen, Theiss and Crüwell, Sophia and Ioannidis, John P. A.},
	month = feb,
	year = {2020},
	note = {Publisher: Royal Society},
	keywords = {meta-research, open science, reproducibility, social sciences, transparency},
	pages = {190806},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/WDAAACNV/Hardwicke et al. - 2020 - An empirical assessment of transparency and reproducibility-related research practices in the social.pdf:application/pdf},
}

@article{hardwicke2022,
	title = {Estimating the {Prevalence} of {Transparency} and {Reproducibility}-{Related} {Research} {Practices} in {Psychology} (2014–2017)},
	volume = {17},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691620979806},
	doi = {10.1177/1745691620979806},
	abstract = {Psychologists are navigating an unprecedented period of introspection about the credibility and utility of their discipline. Reform initiatives emphasize the benefits of transparency and reproducibility-related research practices; however, adoption across the psychology literature is unknown. Estimating the prevalence of such practices will help to gauge the collective impact of reform initiatives, track progress over time, and calibrate future efforts. To this end, we manually examined a random sample of 250 psychology articles published between 2014 and 2017. Over half of the articles were publicly available (154/237, 65\%, 95\% confidence interval [CI] = [59\%, 71\%]); however, sharing of research materials (26/183; 14\%, 95\% CI = [10\%, 19\%]), study protocols (0/188; 0\%, 95\% CI = [0\%, 1\%]), raw data (4/188; 2\%, 95\% CI = [1\%, 4\%]), and analysis scripts (1/188; 1\%, 95\% CI = [0\%, 1\%]) was rare. Preregistration was also uncommon (5/188; 3\%, 95\% CI = [1\%, 5\%]). Many articles included a funding disclosure statement (142/228; 62\%, 95\% CI = [56\%, 69\%]), but conflict-of-interest statements were less common (88/228; 39\%, 95\% CI = [32\%, 45\%]). Replication studies were rare (10/188; 5\%, 95\% CI = [3\%, 8\%]), and few studies were included in systematic reviews (21/183; 11\%, 95\% CI = [8\%, 16\%]) or meta-analyses (12/183; 7\%, 95\% CI = [4\%, 10\%]). Overall, the results suggest that transparency and reproducibility-related research practices were far from routine. These findings establish baseline prevalence estimates against which future progress toward increasing the credibility and utility of psychology research can be compared.},
	language = {en},
	number = {1},
	urldate = {2024-10-09},
	journal = {Perspectives on Psychological Science},
	author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica E. and Wallach, Joshua D. and Kidwell, Mallory C. and Ioannidis, John P. A.},
	month = jan,
	year = {2022},
	note = {Publisher: SAGE Publications Inc},
	pages = {239--251},
	file = {SAGE PDF Full Text:/Users/joelpick/Zotero/storage/T95JYLBY/Hardwicke et al. - 2022 - Estimating the Prevalence of Transparency and Reproducibility-Related Research Practices in Psycholo.pdf:application/pdf},
}

@article{tenney2021,
	title = {Open science and reform practices in organizational behavior research over time (2011 to 2019)},
	volume = {162},
	issn = {0749-5978},
	url = {https://www.sciencedirect.com/science/article/pii/S0749597820304015},
	doi = {10.1016/j.obhdp.2020.10.015},
	abstract = {The “credibility revolution” has fueled a number of initiatives to help bring scientific practices more in line with scientific ideals. These initiatives include increasing the sample size of studies, making data and materials publicly available, pre-registering data collection and analysis plans, publishing replication attempts, and publishing null results. To what extent have these practices become the norm in quantitative Organizational Behavior research? In the current study, using computer algorithms and human coders, we coded the reported use of several open science and reform practices in articles published in four prominent journals (Academy of Management Journal; Journal of Applied Psychology; Organizational Behavior and Human Decision Processes; and Organization Science) from 2011 through 2019. We found that although the vast majority of articles did not use any open science practices, some practices we coded were on the rise, especially in the last two to three years. While there is much room for improvement, these results suggest the field could be on the brink of important and sustained change.},
	urldate = {2024-10-09},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Tenney, Elizabeth R. and Costa, Elaine and Allard, Aurélien and Vazire, Simine},
	month = jan,
	year = {2021},
	keywords = {Credibility revolution, Metascience, Open science, Reform practices},
	pages = {218--223},
	file = {ScienceDirect Snapshot:/Users/joelpick/Zotero/storage/C3YRUZAW/S0749597820304015.html:text/html},
}

@article{scoggins2024,
	title = {Measuring transparency in the social sciences: political science and international relations},
	volume = {11},
	shorttitle = {Measuring transparency in the social sciences},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsos.240313},
	doi = {10.1098/rsos.240313},
	abstract = {The scientific method is predicated on transparency—yet the pace at which transparent research practices are being adopted by the scientific community is slow. The replication crisis in psychology showed that published findings employing statistical inference are threatened by undetected errors, data manipulation and data falsification. To mitigate these problems and bolster research credibility, open data and preregistration practices have gained traction in the natural and social sciences. However, the extent of their adoption in different disciplines is unknown. We introduce computational procedures to identify the transparency of a research field using large-scale text analysis and machine learning classifiers. Using political science and international relations as an illustrative case, we examine 93 931 articles across the top 160 political science and international relations journals between 2010 and 2021. We find that approximately 21\% of all statistical inference papers have open data and 5\% of all experiments are preregistered. Despite this shortfall, the example of leading journals in the field shows that change is feasible and can be effected quickly.},
	number = {7},
	urldate = {2024-10-09},
	journal = {Royal Society Open Science},
	author = {Scoggins, Bermond and Robertson, Matthew P.},
	month = jul,
	year = {2024},
	note = {Publisher: Royal Society},
	keywords = {data sharing, journal policy, open science, preregistration},
	pages = {240313},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/KZYD2W2G/Scoggins and Robertson - 2024 - Measuring transparency in the social sciences political science and international relations.pdf:application/pdf},
}

@misc{hardwicke2024,
	title = {Prevalence of transparent research practices in psychology: {A} cross-sectional study of empirical articles published in 2022},
	shorttitle = {Prevalence of transparent research practices in psychology},
	url = {https://osf.io/t2zs9},
	doi = {10.31234/osf.io/t2zs9},
	abstract = {Over a decade of advocacy and policy reforms have attempted to increase the uptake of transparent research practices in the field of psychology; however, their collective impact is unclear. We estimated the prevalence of transparent research practices in (a) all psychology journals (i.e., field-wide); and (b) prominent psychology journals, by manually examining two random samples of 200 empirical articles (N = 400) published in 2022. Most articles had an open access version (field-wide: 74\%, 95\% confidence intervals [67\%-79\%]; prominent: 71\% [64\%-77\%]) and included a funding statement (field-wide: 76\% [70\%-82\%]; prominent: 76\% [70\%-82\%]); or conflict of interest statement (field-wide: 76\% [70\%-82\%]; prominent: 73\% [67\%-79\%]). Relatively few articles had a preregistration (field-wide: 7\% [2.5\%-12\%]; prominent: 14\% [8.5\%-19\%]); materials (field-wide: 16\% [9\%-24\%]; prominent: 19\% [12\%-27\%]); raw/primary data (field-wide: 14\% [7\%-21\%]; prominent: 16\% [9.5\%-24\%]); or analysis scripts (field-wide: 8.5\% [4.5\%-13\%]; prominent: 14\% [9.5\%-19\%]) that were immediately accessible without contacting authors or third parties. In conjunction with prior research (Hardwicke et al., 2022), our results suggest transparency increased moderately from 2017 to 2022. Overall, despite considerable infrastructure improvements, bottom-up advocacy, and top-down policy initiatives, research transparency continues to be widely neglected in psychology.},
	language = {en-us},
	urldate = {2024-10-09},
	publisher = {OSF},
	author = {Hardwicke, Tom E. and Thibault, Robert T. and Clarke, Beth and Moodie, Nicholas and Crüwell, Sophia and Schiavone, Sarah R. and Handcock, Sarah and Nghiem, Khanh An and Mody, Fallon and Eerola, Tuomas and Vazire, Simine},
	month = jun,
	year = {2024},
	keywords = {data availability, meta-research, open code, open data, open science, preregistration, reproducibility, research transparency},
}

@article{huber2022,
	title = {Nobel and novice: {Author} prominence affects peer review},
	volume = {119},
	shorttitle = {Nobel and novice},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.2205779119},
	doi = {10.1073/pnas.2205779119},
	abstract = {Peer review is a well-established cornerstone of the scientific process, yet it is not immune to biases like status bias, which we explore in this paper. Merton described this bias as prominent researchers getting disproportionately great credit for their contribution, while relatively unknown researchers get disproportionately little credit [R. K. Merton, Science 159, 56–63 (1968)]. We measured the extent of this bias in the peer-review process through a preregistered field experiment. We invited more than 3,300 researchers to review a finance research paper jointly written by a prominent author (a Nobel laureate) and by a relatively unknown author (an early career research associate), varying whether reviewers saw the prominent author’s name, an anonymized version of the paper, or the less-well-known author’s name. We found strong evidence for the status bias: More of the invited researchers accepted to review the paper when the prominent name was shown, and while only 23\% recommended “reject” when the prominent researcher was the only author shown, 48\% did so when the paper was anonymized, and 65\% did when the little-known author was the only author shown. Our findings complement and extend earlier results on double-anonymized vs. single-anonymized review [R. Blank, Am. Econ. Rev. 81, 1041–1067 (1991); M. A. Ucci, F. D’Antonio, V. Berghella, Am. J. Obstet. Gynecol. MFM 4, 100645 (2022)].},
	number = {41},
	urldate = {2024-10-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Huber, Jürgen and Inoua, Sabiou and Kerschbamer, Rudolf and König-Kersting, Christian and Palan, Stefan and Smith, Vernon L.},
	month = oct,
	year = {2022},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2205779119},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/HX4ATD4P/Huber et al. - 2022 - Nobel and novice Author prominence affects peer review.pdf:application/pdf},
}

@article{fox2023,
	title = {Double-blind peer review affects reviewer ratings and editor decisions at an ecology journal},
	volume = {37},
	copyright = {© 2023 The Authors. Functional Ecology © 2023 British Ecological Society.},
	issn = {1365-2435},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1365-2435.14259},
	doi = {10.1111/1365-2435.14259},
	abstract = {There is substantial evidence that systemic biases influence the scholarly peer review process. Many scholars have advocated for double-blind peer review (also known as double-anonymous review) to reduce these biases. However, the effectiveness of double-blind peer review in eliminating biases is uncertain because few randomized trials have manipulated blinding of author identities for journal submissions and those that have are generally small or provide few insights on how it influences reviewer biases. In 2019, Functional Ecology began a large, randomized trial, using real manuscript submissions, to evaluate the various consequences of shifting to double-blind peer review. Research papers submitted to the journal were randomly assigned to be reviewed with author identities blinded to reviewers (double-blind review) or with authors identified to reviewers (single-blind review). In this paper, we explore the effect of blinding on the outcomes of peer review, examining reviewer ratings and editorial decisions, and ask whether author gender and/or location mediate the effects of review type. Double-blind review reduced the average success of manuscripts in peer review; papers reviewed with author identities blinded received on average lower ratings from reviewers and were less likely to be invited for revision or resubmission. However, the effect of review treatment varied with the author's location. Papers with first authors residing in countries with a higher human development index (HDI) and/or higher average English proficiency fared much better than those from countries with a lower HDI and lower English proficiency, but only when author identities were known to reviewers; outcomes were similar between demographic groups when author identities were not known to reviewers. Blinding author identities had no effect on gender differences in reviewer ratings or editor decisions. Our data provide strong evidence that authors from higher income and/or English-speaking countries receive significant benefits (a large positive bias) to being identified to reviewers during the peer review process and that anonymizing author-identities (e.g. double-blind review) reduces this bias, making the peer review process more equitable. We suggest that offering optional blinding of author identities, as some journals allow, is unlikely to substantially reduce the biases that exist because authors from higher-income and English-speaking countries are the least likely to choose to be reviewed with their identity anonymized. Read the free Plain Language Summary for this article on the Journal blog.},
	language = {en},
	number = {5},
	urldate = {2024-10-09},
	journal = {Functional Ecology},
	author = {Fox, Charles W. and Meyer, Jennifer and Aimé, Emilie},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/1365-2435.14259},
	keywords = {double-anonymous peer review, gender bias, prestige bias, single-blind peer review, unconscious bias},
	pages = {1144--1157},
	file = {Snapshot:/Users/joelpick/Zotero/storage/H6NW22YN/1365-2435.html:text/html},
}

@article{cassia-silva2023,
	title = {Overcoming the gender bias in ecology and evolution: is the double-anonymized peer review an effective pathway over time?},
	volume = {11},
	issn = {2167-8359},
	shorttitle = {Overcoming the gender bias in ecology and evolution},
	url = {https://peerj.com/articles/15186},
	doi = {10.7717/peerj.15186},
	abstract = {Male researchers dominate scientific production in science, technology, engineering, and mathematics (STEM). However, potential mechanisms to avoid this gender imbalance remain poorly explored in STEM, including ecology and evolution areas. In the last decades, changes in the peer-review process towards double-anonymized (DA) have increased among ecology and evolution (EcoEvo) journals. Using comprehensive data on articles from 18 selected EcoEvo journals with an impact factor {\textgreater}1, we tested the effect of the DA peer-review process in female-leading (i.e., first and senior authors) articles. We tested whether the representation of female-leading authors differs between double and single-anonymized (SA) peer-reviewed journals. Also, we tested if the adoption of the DA by previous SA journals has increased the representativeness of female-leading authors over time. We found that publications led by female authors did not differ between DA and SA journals. Moreover, female-leading articles did not increase after changes from SA to DA peer-review. Tackling female underrepresentation in science is a complex task requiring many interventions. Still, our results highlight that adopting the DA peer-review system alone could be insufficient in fostering gender equality in EcoEvo scientific publications. Ecologists and evolutionists understand how diversity is important to ecosystems’ resilience in facing environmental changes. The question remaining is: why is it so difficult to promote and keep this “diversity” in addition to equity and inclusion in the academic environment? We thus argue that all scientists, mentors, and research centers must be engaged in promoting solutions to gender bias by fostering diversity, inclusion, and affirmative measures.},
	language = {en},
	urldate = {2024-10-09},
	journal = {PeerJ},
	author = {Cássia-Silva, Cibele and Rocha, Barbbara Silva and Liévano-Latorre, Luisa Fernanda and Sobreiro, Mariane Brom and Diele-Viegas, Luisa Maria},
	month = apr,
	year = {2023},
	note = {Publisher: PeerJ Inc.},
	pages = {e15186},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/BWGKD7DT/Cássia-Silva et al. - 2023 - Overcoming the gender bias in ecology and evolution is the double-anonymized peer review an effecti.pdf:application/pdf},
}

@article{cox2019,
	title = {The cases for and against double-blind reviews},
	volume = {7},
	issn = {2167-8359},
	url = {https://peerj.com/articles/6702},
	doi = {10.7717/peerj.6702},
	abstract = {To date, the majority of authors on scientific publications have been men. While much of this gender bias can be explained by historic sexism and discrimination, there is concern that women may still be disadvantaged by the peer review process if reviewers’ biases lead them to reject publications with female authors more often. One potential solution to this perceived gender bias in the reviewing process is for journals to adopt double-blind reviews whereby neither the authors nor the reviewers are aware of each other’s identity and gender. To test the efficacy of double-blind reviews in one behavioral ecology journal (Behavioral Ecology, BE), we assigned gender to every authorship of every paper published for 2010–2018 in that journal compared to four other journals with single-blind reviews but similar subject matter and impact factors. While female authorships comprised only 35\% of the total in all journals, the double-blind journal (BE) did not have more female authorships than its single-blind counterparts. Interestingly, the incidence of female authorship is higher at behavioral ecology journals (BE and Behavioral Ecology and Sociobiology) than in the ornithology journals (Auk, Condor, Ibis) for papers on all topics as well as those on birds. These analyses suggest that double-blind review does not currently increase the incidence of female authorship in the journals studied here. We conclude, at least for these journals, that double-blind review no longer benefits female authors and we discuss the pros and cons of the double-blind reviewing process based on our findings.},
	language = {en},
	urldate = {2024-10-09},
	journal = {PeerJ},
	author = {Cox, Amelia R. and Montgomerie, Robert},
	month = apr,
	year = {2019},
	note = {Publisher: PeerJ Inc.},
	pages = {e6702},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/R9VK6KE2/Cox and Montgomerie - 2019 - The cases for and against double-blind reviews.pdf:application/pdf},
}

@article{borja2015,
	title = {Is there gender bias in the peer-review process in several {Elsevier}’s marine journals?},
	volume = {96},
	issn = {0025-326X},
	url = {https://www.sciencedirect.com/science/article/pii/S0025326X15003264},
	doi = {10.1016/j.marpolbul.2015.05.046},
	number = {1},
	urldate = {2024-10-09},
	journal = {Marine Pollution Bulletin},
	author = {Borja, Ángel},
	month = jul,
	year = {2015},
	pages = {1--2},
	file = {ScienceDirect Snapshot:/Users/joelpick/Zotero/storage/I9QZMXLE/S0025326X15003264.html:text/html},
}

@article{fox2019,
	title = {Gender differences in peer review outcomes and manuscript impact at six journals of ecology and evolution},
	volume = {9},
	copyright = {© 2019 The Authors. Ecology and Evolution published by John Wiley \& Sons Ltd.},
	issn = {2045-7758},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ece3.4993},
	doi = {10.1002/ece3.4993},
	abstract = {The productivity and performance of men is generally rated more highly than that of women in controlled experiments, suggesting conscious or unconscious gender biases in assessment. The degree to which editors and reviewers of scholarly journals exhibit gender biases that influence outcomes of the peer-review process remains uncertain due to substantial variation among studies. We test whether gender predicts the outcomes of editorial and peer review for {\textgreater}23,000 research manuscripts submitted to six journals in ecology and evolution from 2010 to 2015. Papers with female and male first authors were equally likely to be sent for peer review. However, papers with female first authors obtained, on average, slightly worse peer-review scores and were more likely to be rejected after peer review, though the difference varied among journals. These gender differences appear to be partly due to differences in authorial roles. Papers for the which the first author deferred corresponding authorship to a coauthor (which women do more often than men) obtained significantly worse peer-review scores and were less likely to get positive editorial decisions. Gender differences in corresponding authorship explained some of the gender differences in peer-review scores and positive editorial decisions. In contrast to these observations on submitted manuscripts, gender differences in peer-review outcomes were observed in a survey of {\textgreater}12,000 published manuscripts; women reported similar rates of rejection (from a prior journal) before eventual publication. After publication, papers with female authors were cited less often than those with male authors, though the differences are very small ( 2\%). Our data do not allow us to test hypotheses about mechanisms underlying the gender discrepancies we observed, but strongly support the conclusion that papers authored by women have lower acceptance rates and are less well cited than are papers authored by men in ecology.},
	language = {en},
	number = {6},
	urldate = {2024-10-09},
	journal = {Ecology and Evolution},
	author = {Fox, Charles W. and Paine, C. E. Timothy},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ece3.4993},
	keywords = {bias, citations, discrimination, gender, peer review, scholarly publishing},
	pages = {3599--3619},
	file = {Full Text PDF:/Users/joelpick/Zotero/storage/WXDIATN4/Fox and Paine - 2019 - Gender differences in peer review outcomes and manuscript impact at six journals of ecology and evol.pdf:application/pdf;Snapshot:/Users/joelpick/Zotero/storage/VNBE4IRR/ece3.html:text/html},
}

@article{engqvist2008,
	title = {Double-blind peer review and gender publication bias},
	volume = {76},
	issn = {0003-3472},
	url = {https://www.sciencedirect.com/science/article/pii/S0003347208002601},
	doi = {10.1016/j.anbehav.2008.05.023},
	number = {3},
	urldate = {2024-10-09},
	journal = {Animal Behaviour},
	author = {Engqvist, Leif and Frommen, Joachim G.},
	month = sep,
	year = {2008},
	keywords = {effect size, gender bias, GLMM, refereeing process, sex difference},
	pages = {e1--e2},
	file = {Full Text:/Users/joelpick/Zotero/storage/WR5FBHF6/Engqvist and Frommen - 2008 - Double-blind peer review and gender publication bias.pdf:application/pdf;ScienceDirect Snapshot:/Users/joelpick/Zotero/storage/5Q8VDNPS/S0003347208002601.html:text/html},
}

@article{whittaker2008,
	title = {Journal review and gender equality: a critical comment on {Budden} et al.},
	volume = {23},
	issn = {0169-5347},
	shorttitle = {Journal review and gender equality},
	url = {https://www.cell.com/trends/ecology-evolution/abstract/S0169-5347(08)00223-1},
	doi = {10.1016/j.tree.2008.06.003},
	language = {English},
	number = {9},
	urldate = {2024-10-09},
	journal = {Trends in Ecology \& Evolution},
	author = {Whittaker, Robert J.},
	month = sep,
	year = {2008},
	pmid = {18640741},
	note = {Publisher: Elsevier},
	pages = {478--479},
}

@article{webb2008,
	title = {Does double-blind review benefit female authors?},
	volume = {23},
	issn = {0169-5347},
	url = {https://www.cell.com/trends/ecology-evolution/abstract/S0169-5347(08)00138-9},
	doi = {10.1016/j.tree.2008.03.003},
	language = {English},
	number = {7},
	urldate = {2024-10-09},
	journal = {Trends in Ecology \& Evolution},
	author = {Webb, Thomas J. and O’Hara, Bob and Freckleton, Robert P.},
	month = jul,
	year = {2008},
	pmid = {18450323},
	note = {Publisher: Elsevier},
	pages = {351--353},
}

@article{budden2008,
	title = {Double-blind review favours increased representation of female authors},
	volume = {23},
	issn = {0169-5347},
	url = {https://www.sciencedirect.com/science/article/pii/S0169534707002704},
	doi = {10.1016/j.tree.2007.07.008},
	abstract = {Double-blind peer review, in which neither author nor reviewer identity are revealed, is rarely practised in ecology or evolution journals. However, in 2001, double-blind review was introduced by the journal Behavioral Ecology. Following this policy change, there was a significant increase in female first-authored papers, a pattern not observed in a very similar journal that provides reviewers with author information. No negative effects could be identified, suggesting that double-blind review should be considered by other journals.},
	number = {1},
	urldate = {2024-10-09},
	journal = {Trends in Ecology \& Evolution},
	author = {Budden, Amber E. and Tregenza, Tom and Aarssen, Lonnie W. and Koricheva, Julia and Leimu, Roosa and Lortie, Christopher J.},
	month = jan,
	year = {2008},
	pages = {4--6},
	file = {ScienceDirect Snapshot:/Users/joelpick/Zotero/storage/RVMZHFYP/S0169534707002704.html:text/html},
}
